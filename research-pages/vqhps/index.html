<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="VQ-HPS addresses the human pose and shape estimation problem in a vector-quantized latent space.">
    <meta name="keywords" content="VQ-HPS, human pose and shape estimation, vector quantization">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space</title>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/digisport.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">VQ-HPS: Human Pose and Shape Estimation in a
                            Vector-Quantized Latent Space
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://g-fiche.github.io/">Guénolé Fiche</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://sleglaive.github.io/">Simon Leglaive</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://xavirema.eu/">Xavier Alameda-Pineda</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.iri.upc.edu/people/aagudo/">Antonio Agudo</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.iri.upc.edu/people/fmoreno/">Francesc
                                    Moreno-Noguer</a><sup>3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>CentraleSupélec, IETR UMR CNRS 6164, France - </span>
                            <span class="author-block"><sup>2</sup>Inria, Univ. Grenoble Alpes, CNRS, LJK, France -
                            </span>
                            <span class="author-block"><sup>3</sup>Institut de Robòtica i Informàtica Industrial,
                                CSIC-UPC, Spain</span>
                        </div>

                        <br>

                        <div class="is-size-5">
                            European Conference on Computer Vision (ECCV) 2024
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="static/data/VQ-HPS_arxiv.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/g-fiche/Mesh-VQ-VAE"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code Mesh-VQ-VAE</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/g-fiche/VQ-HPS"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code VQ-HPS</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <div class="columns is-centered">
        <table>
            <td> <img src="./static/images/teaser-image.png" alt="Teaser" width="1000" /> </td>
        </table>
    </div>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Previous works on Human Pose and Shape Estimation (HPSE) from RGB images can be broadly
                            categorized into two main groups: parametric and non-parametric approaches. Parametric
                            techniques leverage a low-dimensional statistical body model for realistic results, whereas
                            recent non-parametric methods achieve higher precision by directly regressing the 3D
                            coordinates of the human body mesh.
                            This work introduces a novel paradigm to address the HPSE problem, involving a
                            low-dimensional discrete latent representation of the human mesh and framing HPSE as a
                            classification task. Instead of predicting body model parameters or 3D vertex coordinates,
                            we focus on predicting the proposed discrete latent representation, which can be decoded
                            into a registered human mesh. This innovative paradigm offers two key advantages. Firstly,
                            predicting a low-dimensional discrete representation confines our predictions to the space
                            of anthropomorphic poses and shapes even when little training data is available. Secondly,
                            by framing the problem as a classification task, we can harness the discriminative power
                            inherent in neural networks.
                            The proposed model, VQ-HPS, predicts the discrete latent representation of the mesh. The
                            experimental results demonstrate that VQ-HPS outperforms the current state-of-the-art
                            non-parametric approaches while yielding results as realistic as those produced by
                            parametric methods when trained with few data. VQ-HPS also shows promising results when
                            training on large-scale datasets, highlighting the significant potential of the
                            classification approach for HPSE.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <h2 class="title is-3">Architecture of the model</h2>
            </div>

            <br>

            <div class="columns is-centered">
                <table>
                    <td> <img src="./static/images/VQ-HPS_full_dim.png" alt="Architecture" width="1000" /> </td>
                </table>
            </div>

            <br>

            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <div class="content has-text-justified">
                        <p>
                            This work introduces a method significantly different from all prior human pose and shape
                            estimation (HPSE) approaches. Instead of predicting the parameters of a human body model or
                            3D coordinates, we learn to predict a learned discrete latent representation of 3D meshes,
                            transforming the HPSE into a classification problem.
                        </p>
                        <p>
                            For learning our discrete latent representation of meshes, we build on the <a
                                href="https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf">vector
                                quantized-variational autoencoder (VQVAE)</a> framework and adapt it to
                            the <a href="https://zhouyisjtu.github.io/project_vcmeshcnn/vcmeshcnn.html">fully
                                convolutional mesh autoencoder</a>. The encoder of
                            the proposed model, called Mesh-VQ-VAE, provides a low-dimensional discrete latent
                            representation preserving the spatial structure of the mesh.
                        </p>
                        <p>
                            We then propose a Transformer-based encoder-decoder model, called VQ-HPS, for predicting the
                            human mesh discrete representation of the introduced Mesh-VQ-VAE from image features.
                            To ease the low-dimensional representation learning of the mesh, the predicted mesh is
                            non-oriented and centered on the origin: we call it a canonical mesh. To obtain the final
                            oriented mesh, we then need to predict the rotation , and for better alignment with the
                            image, we also regress the
                            weak-perspective camera.
                        </p>
                    </div>
                </div>
            </div>

            <br>
            <br>

            <div class="columns is-centered has-text-centered">
                <h2 class="title is-3">Mesh-VQ-VAE</h2>
            </div>

            <br>

            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <div class="content has-text-justified">
                        <p>
                            The Mesh-VQ-VAE is trained on the <a href="https://amass.is.tue.mpg.de">AMASS</a>
                            dataset and finetuned on the <a href="https://virtualhumans.mpi-inf.mpg.de/3DPW/">3DPW</a>
                            training set. The final reconstruction
                            error is 4.7mm. Some reconstruction samples from the 3DPW dataset can be found below.
                        </p>
                    </div>
                </div>
            </div>

            <br>

            <div class="columns is-centered">
                <table>
                    <td> <img src="./static/images/reconstruction.png" alt="reconstruction" width="1000" /> </td>
                </table>
            </div>

            <br>

            <!-- Scarce data -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Training on scarce data</h2>
                    <div class="content has-text-justified">
                        <p>
                            We train VQ-HPS separately on the <a
                                href="https://virtualhumans.mpi-inf.mpg.de/3DPW/">3DPW</a>,
                            <a href="https://cocodataset.org/#home">COCO</a>, and <a
                                href="https://eth-ait.github.io/emdb/">EMDB</a> training sets
                            to see how it performs when trained on limited data. We compare our performance with
                            <a href="https://akanazawa.github.io/hmr/">HMR</a>, <a
                                href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650580.pdf">Cliff</a>,
                            and <a href="https://fastmetro.github.io/">FastMETRO</a>
                            trained with the same data. Qualitative results can be found below.
                        </p>
                    </div>
                </div>
            </div>

            <br>

            <div class="columns is-centered">
                <table>
                    <td> <img src="./static/images/comparison.png" alt="Scarce" width="1000" /> </td>
                </table>
            </div>

            <br>

            <!-- Real data -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Training on large-scale datasets</h2>
                    <div class="content has-text-justified">
                        <p>
                            Following the standard practice, we
                            train VQ-HPS on <a href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</a>,
                            <a href="https://vcai.mpi-inf.mpg.de/3dhp-dataset/">MPI-INF-3DHP</a>,
                            <a href="https://cocodataset.org/#home">COCO</a>,
                            and <a href="http://human-pose.mpi-inf.mpg.de/">MPII</a>.
                            Qualitative results on the <a href="https://virtualhumans.mpi-inf.mpg.de/3DPW/">3DPW</a>
                            dataset
                            are available below. For video results, the frames are processed indepently but we apply a
                            linear moving average with a window of size 10 on vertices to smooth the results.
                        </p>
                    </div>
                </div>
            </div>

            <br>

            <div class="columns is-centered">
                <table>
                    <td> <img src="./static/images/viz_real.png" alt="Real" width="1000" /> </td>
                </table>
            </div>

            <br>

            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <video id="vid1" autoplay muted loop playsinline height="100%">
                        <source src="./static/videos/outdoors_warmup.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <br>

            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <video id="vid2" autoplay muted loop playsinline height="100%">
                        <source src="./static/videos/outdoors_fencing.mp4" type="video/mp4">
                    </video>
                </div>
            </div>


        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@inproceedings{fiche2024vq,
                title={{VQ-HPS}: Human Pose and Shape Estimation in a Vector-Quantized Latent Space},
                author={Fiche, Gu{\'e}nol{\'e} and Leglaive, Simon and Alameda-Pineda, Xavier and Agudo, Antonio and Moreno-Noguer, Francesc},
                booktitle={European Conference on Computer Vision ({ECCV})},
                year={2024}
              }</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        Website source code borrowed from <a href="https://keunhong.com/">Keunhong Park</a>'s <a
                            href="https://nerfies.github.io/">Nerfies</a> website.
                    </p>
                </div>
            </div>
            <br>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        This study is part of the <a href="https://digisport.univ-rennes.fr/en/home/">EUR DIGISPORT</a>
                        project
                        supported by the ANR within the framework of the PIA France
                        2030 (ANR-18-EURE-0022). This work was performed using HPC resources from the <a
                            href="http://mesocentre.centralesupelec.fr/">"Mésocentre"</a> computing center
                        of CentraleSupélec, École Normale Supérieure Paris-Saclay, and Université Paris-Saclay supported
                        by CNRS and
                        Région Île-de-France.
                    </p>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>