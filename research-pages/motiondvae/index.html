<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Motion-DVAE denoise mocap data through an unsupervised learned denoising.">
  <meta name="keywords" content="Motion-DVAE, prior, DVAE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Motion-DVAE: Unsupervised learning for fast human motion denoising</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/digisport.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9LTDGDQHFV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9LTDGDQHFV');
  </script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Motion-DVAE: Unsupervised learning for fast human motion denoising
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://g-fiche.github.io/">Guénolé Fiche</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://sleglaive.github.io/">Simon Leglaive</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://xavirema.eu/">Xavier Alameda-Pineda</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.ietr.fr/renaud-seguier">Renaud Séguier</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CentraleSupélec, IETR UMR CNRS 6164, France - </span>
              <span class="author-block"><sup>2</sup>Inria, Univ. Grenoble Alpes, CNRS, LJK, France</span>
            </div>

            <br>

            <div class="is-size-5">
              ACM MIG 2023
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3623264.3624454" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="static/data/Motion_DVAE_Supplementary.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary material</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://g-fiche.github.io/research-pages/motiondvae/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Pose and motion priors are crucial for recovering realistic and accurate human motion from noisy
              observations.
              Substantial progress has been made on pose and shape estimation from images, and recent works showed
              impressive results using priors to refine frame-wise
              predictions. However, a lot of motion priors only model transitions between consecutive poses and are used
              in time-consuming optimization procedures,
              which is problematic for many applications requiring real-time motion capture. We introduce Motion-DVAE,
              a motion prior to capture the short-term dependencies of human motion. As part of the dynamical
              variational autoencoder (DVAE) models family,
              Motion-DVAE combines the generative capability of VAE models and the temporal modeling of recurrent
              architectures. Together with Motion-DVAE,
              we introduce an unsupervised learned denoising method unifying regression- and optimization-based
              approaches in a single framework
              for real-time 3D human pose estimation. Experiments show that the proposed approach reaches competitive
              performance with state-of-the-art methods
              while being much faster.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Accuracy versus speed</h2>
      </div>

      <br>

      <div class="columns is-centered">
        <table>
          <td> <img src="./static/images/evolution.png" alt="Motion-DVAE" width="1000" /> </td>
        </table>
      </div>

      <br>

      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              We compare Motion-DVAE with state-of-the-art methods in terms of accuracy (V2V error) and plausibility
              (joints acceleration).
              Our method is much faster than others while showing comparable results at the end of optimization.
            </p>
          </div>
        </div>
      </div>

      <br>
      <br>

      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Qualitative results</h2>
      </div>

      <br>

      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              First, we visualise results for motion capture data denoising. Noisy observations are obtained by adding
              random noise in
              the pose parameter of sequences of the AMASS dataset.
              Motion-DVAE produces realistic motions, close to the groundtruth despite significant average noise in
              observations
              (12.89 cm).
              Optimization slightly improves the results compare to regression, particularly for arms motion and head
              orientation.
            </p>
          </div>
        </div>
      </div>

      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/box/box.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/throw/throw.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <br>

      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              We also visualise predictions on the i3DB dataset. Even when projected in 2D, our predictions match visual
              cues, even if we do not have any reprojection loss.
              We can observe small breaks in Motion-DVAE's results. This is due to the concatenation of
              subsequences predicted independently.
            </p>
          </div>
        </div>
      </div>

      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/scene05/scene05.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/scene13/scene13.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>
          <div class="content has-text-justified">
            <p>
              Visualisations of this website are based on several works.
            </p>
            <p>
              <a href="https://amass.is.tue.mpg.de">AMASS</a> is a large database of human motion unifying
              different motion capture datasets in
              <a href="https://smpl.is.tue.mpg.de/">SMPL</a> format.
              All motions for noisy motion capture denoising are generated from this database.
            </p>
            <p>
              The <a href="https://geometry.cs.ucl.ac.uk/projects/2019/imapper/">i3DB</a> dataset provides
              videos with rich 3D annotations on
              3D pose and scene geometry. We use this dataset for motion estimation from RGB videos.
            </p>
            <p>
              The code to generate visualisations is inspired from the code of <a
                href="https://geometry.stanford.edu/projects/humor/">HuMoR</a>.
            </p>
            <p>
              Comparison with state-of-the-art methods were done using the public implementations of <a
                href="https://github.com/mkocabas/VIBE">VIBE</a>, <a
                href="https://geometry.stanford.edu/projects/humor/">HuMoR</a> and <a
                href="https://virtualhumans.mpi-inf.mpg.de/posendf/">Pose-NDF</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{Fiche23MotionDVAE,
        author    = {Fiche, Guénolé and Leglaive, Simon and Alameda-Pineda, Xavier and Séguier, Renaud},
        title     = {Motion-DVAE: Unsupervised learning for fast human motion denoising},
        journal   = {ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG)},
        year      = {2023}
      }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="content">
          <p>
            Website source code borrowed from <a href="https://keunhong.com/">Keunhong Park</a>'s <a
              href="https://nerfies.github.io/">Nerfies</a> website.
          </p>
        </div>
      </div>
      <br>
      <div class="columns is-centered">
        <div class="content">
          <p>
            This study is part of the <a href="https://digisport.univ-rennes.fr/en/home/">EUR DIGISPORT</a> project
            supported by the ANR within the framework of the PIA France
            2030 (ANR-18-EURE-0022). This work was performed using HPC resources from the <a
              href="http://mesocentre.centralesupelec.fr/">"Mésocentre"</a> computing center
            of CentraleSupélec, École Normale Supérieure Paris-Saclay, and Université Paris-Saclay supported by CNRS and
            Région Île-de-France.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>